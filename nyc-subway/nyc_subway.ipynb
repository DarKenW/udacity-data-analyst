{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the NYC Subway Dataset\n",
    "================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secction 0. References\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Welch's t test](http://en.wikipedia.org/wiki/Welch%27s_t_test)\n",
    "* [Mann-Whitney U test](http://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)\n",
    "* [Coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1. Statistical Test\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of riders on rainy day:  1105.44637675\n",
      "Mean number of riders on non-rainy day:  1090.27878015\n",
      "p-value for Mann-Whitney U test:  0.0249999127935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import scipy.stats\n",
    "\n",
    "df = pandas.read_csv(\"data/turnstile_data_master_with_weather.csv\")\n",
    "\n",
    "rain_entries = df[df.rain==1].ENTRIESn_hourly\n",
    "no_rain_entries = df[df.rain==0].ENTRIESn_hourly\n",
    "\n",
    "rain_mean = np.mean(rain_entries)\n",
    "no_rain_mean = np.mean(no_rain_entries)\n",
    "U, p = scipy.stats.mannwhitneyu(rain_entries, no_rain_entries)\n",
    "\n",
    "print \"Mean number of riders on rainy day: \", rain_mean\n",
    "print \"Mean number of riders on non-rainy day: \", no_rain_mean\n",
    "print \"p-value for Mann-Whitney U test: \", p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "\n",
    "I used Mann-Whitney U Test to analyze the NYC subway data.\n",
    "\n",
    "I used a two-tail P value.\n",
    "\n",
    "The null hypothesis is the subway riders are not affected by whether it rains or not, i.e. the number of riders on rainy days and number of riders on non-rainy days follow the same distribution.\n",
    "\n",
    "The P-critical value I use is 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "\n",
    "The Mann-Whitney U Test is applicable to the dataset is a nonparametric test that does not make any assumption on the underlying distribution. By ploting the histogram of two ridership samples, we see they are clearly not normal distribution, and therefore a t test is not suitable for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "The results we get from this statistical test is that the ridership **is** affected by whether it rains or not, in other words, the number of riders on rainy days and number of riders on non-rainy days **do not** follow the same distribution. Furthermore, the number of riders on a rainy day is slightly larger than that on a non-rainy day.\n",
    "\n",
    "The p-value of this test is 0.025. The mean number of riders on rainy days is 1105, and that on non-rainy days is 1090."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "\n",
    "The significance level of this test is 5% (the P-critical value I set in 1.1). The interpretation is that the probability of our conclusion is wrong because of bad luck is less than 5%. More specifically, if the ridership is not affected by raining or not, the probability of seeing data as extreme as the one we currently have is less than 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 2. Linear Regression\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "I used both (a) OLS with statsmodels and (b) gradient descent with skikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def linear_regression_ols(features, values):\n",
    "    \"\"\"\n",
    "    Perform linear regression given a data set with an arbitrary number of features\n",
    "    using ordinary least squares.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = sm.add_constant(features)\n",
    "    model = sm.OLS(values, features)\n",
    "    results = model.fit()\n",
    "    intercept = results.params[0]\n",
    "    params = results.params[1:]\n",
    "    \n",
    "    return intercept, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "def normalize_features(features):\n",
    "    ''' \n",
    "    Returns the means and standard deviations of the given features, along with a normalized feature\n",
    "    matrix.\n",
    "    ''' \n",
    "    means = np.mean(features, axis=0)\n",
    "    std_devs = np.std(features, axis=0)\n",
    "    normalized_features = (features - means) / std_devs\n",
    "    return means, std_devs, normalized_features\n",
    "\n",
    "\n",
    "def recover_params(means, std_devs, norm_intercept, norm_params):\n",
    "    ''' \n",
    "    Recovers the weights for a linear model given parameters that were fitted using\n",
    "    normalized features. Takes the means and standard deviations of the original\n",
    "    features, along with the intercept and parameters computed using the normalized\n",
    "    features, and returns the intercept and parameters that correspond to the original\n",
    "    features.\n",
    "    ''' \n",
    "    intercept = norm_intercept - np.sum(means * norm_params / std_devs)\n",
    "    params = norm_params / std_devs\n",
    "    return intercept, params\n",
    "\n",
    "\n",
    "def linear_regression_sgd(features, values, n_iter=20):\n",
    "    \"\"\"\n",
    "    Perform linear regression given a data set with an arbitrary number of features\n",
    "    using stochastic gradient descent algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    means, std_devs, normalized_features = normalize_features(features)\n",
    "    clf = SGDRegressor(n_iter=n_iter)\n",
    "    fit_result = clf.fit(normalized_features, values)\n",
    "    norm_intercept = fit_result.intercept_[0]\n",
    "    norm_params = fit_result.coef_\n",
    "    intercept, params = recover_params(means, std_devs, norm_intercept, norm_params)\n",
    "    \n",
    "    return intercept, params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "\n",
    "I use `hour`, `weekday` and `meantempi` input variables together with `UNIT` and `conds` as features for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients of non-dummy features:  [ 123.47256977  948.39682638  -21.12298936]\n",
      "Coefficient of determination (R^2): 0.476284384231\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "def compute_r_squared(data, predictions):    \n",
    "    SST = ((data - np.mean(data)) ** 2).sum()\n",
    "    SSReg = ((predictions-data) ** 2).sum()\n",
    "    r_squared = 1 - SSReg / SST\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "\n",
    "df = pandas.read_csv(\"data/turnstile_weather_v2.csv\")\n",
    "\n",
    "# Select features\n",
    "features = df[['hour', 'weekday', 'meantempi']]\n",
    "dummy_units = pandas.get_dummies(df['UNIT'], prefix='unit')\n",
    "features = features.join(dummy_units)\n",
    "dummy_conds = pandas.get_dummies(df['conds'], prefix='conds')\n",
    "features = features.join(dummy_conds)\n",
    "\n",
    "# Values\n",
    "values = df['ENTRIESn_hourly']\n",
    "\n",
    "# Perform linear regression\n",
    "intercept, params = linear_regression_ols(features.values, values.values)\n",
    "predictions = intercept + np.dot(features.values, params)\n",
    "\n",
    "predictions = predict(df)\n",
    "r2 = compute_r_squared(values, predictions)\n",
    "\n",
    "print \"Coefficients of non-dummy features: \", params[:3]\n",
    "print \"Coefficient of determination (R^2):\", r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "I decide to use `weekday` and `hour` because I think the number of riders should be strongly affect by time, e.g. there should be more riders on weekday rush hours (say Monday 9am) than other times (say Saturday 4pm).\n",
    "\n",
    "Similarly, I use `meantempi` and `conds` because I think the weather should also affect number of riders, e.g. there might be more riders on a cold rainy day than a warm sunny day.\n",
    "\n",
    "Finally, I find that `UNIT` has biggest impact to the ridership as adding it significantly boost the R<sup>2</sup> value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4\n",
    "\n",
    "The coefficients for non-dummy features are `[ 123.47256977  948.39682638  -21.12298936]`, corresponding to `hour`, `weekday` and `meantempi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5\n",
    "\n",
    "The R<sup>2</sup> value of the model is `0.47628`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6\n",
    "\n",
    "Having a large R<sup>2</sup> value (close to `1.0`) indicates there is strong linear relationship between the features we chose and the ridership, and vice ver sa. I think a R<sup>2</sup> value of `0.47628` is relatively small, because we are using relatively small number of features to explain the complicated ridership trend. I think the R<sup>2</sup> value we get is reasonable, and it suggests linear model might not be the most appropriate one and a more complicated model is likely to yield better prediction results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 3. Visualization\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4. Conclusion\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 5. Reflection\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
